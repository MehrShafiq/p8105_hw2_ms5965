p8105\_hw2\_ms5965
================
Mehr Shafiq

Loading libraries.

``` r
library(tidyverse)
```

    ## -- Attaching packages -- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ----- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(readxl)
```

## Problem 1

Defining a path to the dataset.

``` r
path_to_data = "./data/trash_wheel_data.xlsx"
```

Reading Mr Trashwheel Data.

``` r
trashwheel_df = 
    read_xlsx(
        path = path_to_data,
        sheet = "Mr. Trash Wheel",
        range = cell_cols("A:N")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster) %>% 
    mutate(
        sports_balls = round(sports_balls),
        sports_balls = as.integer(sports_balls))
```

Reading and cleaning precipitation data for 2017 & 2018

``` r
prec_17 = 
  read_excel(
    path = path_to_data,
    sheet = "2017 Precipitation",
    skip = 1) %>% 
  
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017) %>% 
  relocate (year)

prec_18 = 
  read_excel(
    path = path_to_data,
    sheet = "2018 Precipitation",
    skip = 1) %>% 
  
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018) %>% 
  relocate (year)
```

Combining 2017 & 2018 Precipitation Datasets

``` r
month_df = 
    tibble(
        month = 1:12,
        month_name = month.name)

prec_17_18 = bind_rows(prec_17, prec_18) 

prec_17_18 = left_join(prec_17_18, month_df, by = "month")
```

This dataset contains information from the Mr. Trashwheel trash
collector in Baltimore, Maryland. As trash enters the inner harbor, the
trashwheel collects that trash, and stores it in a dumpster. The dataset
contains information on year, month, and trash collected, include some
specific kinds of trash. There are a total of 344 observations in our
final dataset. Additional data sheets include month precipitation data.
The median number of sports balls found in a dumpster in 2017 was 8. The
total precipitation in 2018 was 70.33 inches.

## Problem 2

Defining a path to the dataset, then cleaning it..

``` r
path_to_NYC_data = read_csv(file = "./data/NYC_subway_data.csv") %>% 
janitor::clean_names() 
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

``` r
subset = c("line", "entrance_latitude", "entrance_longitude", "route1", "route2", "route3", "route4", "route5", "route6", "route7", "route8", "route9", "route10", "route11", "entry", "vending", "entrance_type", "ada", "station_name")

subset_NYC_data = path_to_NYC_data[subset] %>% 
  
mutate(
  entry = ifelse(entry == "YES", TRUE, FALSE)) %>% 
  
mutate_at(vars(route1:route11), as.character) %>%

pivot_longer(
    route1:route11, 
    names_to = "route_name",
    names_prefix = "route",
    values_to = "route_number") %>% 
drop_na(route_number)
```

This dataset contains information on the NYC Transit - with respect to
the entrance, exit, routes, lines and ADA compliance for each subway in
the city.

The unadulterated dataset was cleaned in the following steps: \* A
subset of the dataset with only relevant variables was created. \* The
‘entry’ variable which had YES & NO values was converted into a
logical variables where YES -\> TRUE and NO -\> FALSE. \* The multiple
route names (route1 to route11) were all converted in the same type of
variable, that is, character variable. \* The originally wide dataset
was changed into a long table where the umbrella variable “route\_name”
stores all the route numbers.

This data has 10 variables and 4270 observations.

This dataset is tidy.
